{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89hdk3d7N-hb"
      },
      "source": [
        "##### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cellView": "form",
        "id": "IbX4a6NKOBTr"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYNno0xtOFJ-"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google-research/simclr/blob/master/tf2/colabs/distillation_self_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9oIl-rCOypT"
      },
      "source": [
        "## SimCLR: A Simple Framework for Contrastive Learning of Visual Representations\n",
        "\n",
        "This colab demonstrates how to load pretrained/finetuned SimCLR models from hub modules for distillation / self-training (without needing actual labels).\n",
        "\n",
        "The checkpoints are accessible in the following Google Cloud Storage folders.\n",
        "\n",
        "* Pretrained SimCLRv2 models with a linear classifier: [gs://simclr-checkpoints-tf2/simclrv2/pretrained](https://console.cloud.google.com/storage/browser/simclr-checkpoints-tf2/simclrv2/pretrained)\n",
        "* Fine-tuned SimCLRv2 models on 1% of labels: [gs://simclr-checkpoints-tf2/simclrv2/finetuned_1pct](https://console.cloud.google.com/storage/browser/simclr-checkpoints-tf2/simclrv2/finetuned_1pct)\n",
        "* Fine-tuned SimCLRv2 models on 10% of labels: [gs://simclr-checkpoints-tf2/simclrv2/finetuned_10pct](https://console.cloud.google.com/storage/browser/simclr-checkpoints-tf2/simclrv2/finetuned_10pct)\n",
        "* Fine-tuned SimCLRv2 models on 100% of labels: [gs://simclr-checkpoints-tf2/simclrv2/finetuned_100pct](https://console.cloud.google.com/storage/browser/simclr-checkpoints-tf2/simclrv2/finetuned_100pct)\n",
        "* Supervised models with the same architectures: [gs://simclr-checkpoints-tf2/simclrv2/pretrained](https://console.cloud.google.com/storage/browser/simclr-checkpoints-tf2/simclrv2/pretrained)\n",
        "\n",
        "Use the corresponding checkpoint / hub-module paths for accessing the model. For example, to use a pre-trained model (with a linear classifier) with ResNet-152 (2x+SK), set the path to `gs://simclr-checkpoints-tf2/simclrv2/pretrained/r152_2x_sk1`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ih5NlvdDEOI1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-06 13:07:24.218511: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-02-06 13:07:24.953920: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:\n",
            "2024-02-06 13:07:24.954206: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:\n",
            "2024-02-06 13:07:24.954217: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "/home/wenuka/.pyenv/versions/simclr/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow.compat.v2 as tf\n",
        "tf.compat.v1.enable_v2_behavior()\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "import pydicom\n",
        "import glob\n",
        "import os\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnyvq6g-P2rW",
        "outputId": "73813ef7-88f2-42c3-9172-084cd83f9412"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-02-06 13:07:31--  https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 2404:6800:4003:c1a::cf, 2404:6800:4003:c1c::cf, 2404:6800:4003:c03::cf, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|2404:6800:4003:c1a::cf|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21675 (21K) [text/plain]\n",
            "Saving to: ‘ilsvrc2012_wordnet_lemmas.txt.3’\n",
            "\n",
            "ilsvrc2012_wordnet_ 100%[===================>]  21.17K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-02-06 13:07:32 (418 KB/s) - ‘ilsvrc2012_wordnet_lemmas.txt.3’ saved [21675/21675]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@title Load class id to label text mapping from big_transfer (hidden)\n",
        "# Code snippet credit: https://github.com/google-research/big_transfer\n",
        "\n",
        "!wget --no-check-certificate https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt\n",
        "\n",
        "imagenet_int_to_str = {}\n",
        "\n",
        "with open('ilsvrc2012_wordnet_lemmas.txt', 'r') as f:\n",
        "  for i in range(1000):\n",
        "    row = f.readline()\n",
        "    row = row.rstrip()\n",
        "    imagenet_int_to_str.update({i: row})\n",
        "\n",
        "tf_flowers_labels = ['dandelion', 'daisy', 'tulips', 'sunflowers', 'roses']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BxhfMmVdHoZM"
      },
      "outputs": [],
      "source": [
        "#@title Preprocessing functions from data_util.py in SimCLR repository (hidden).\n",
        "\n",
        "FLAGS_color_jitter_strength = 0.3\n",
        "CROP_PROPORTION = 0.875  # Standard for ImageNet.\n",
        "\n",
        "\n",
        "def random_apply(func, p, x):\n",
        "  \"\"\"Randomly apply function func to x with probability p.\"\"\"\n",
        "  return tf.cond(\n",
        "      tf.less(tf.random.uniform([], minval=0, maxval=1, dtype=tf.float32),\n",
        "              tf.cast(p, tf.float32)),\n",
        "      lambda: func(x),\n",
        "      lambda: x)\n",
        "\n",
        "def random_brightness(image, max_delta, impl='simclrv2'):\n",
        "  \"\"\"A multiplicative vs additive change of brightness.\"\"\"\n",
        "  if impl == 'simclrv2':\n",
        "    factor = tf.random.uniform(\n",
        "        [], tf.maximum(1.0 - max_delta, 0), 1.0 + max_delta)\n",
        "    image = image * factor\n",
        "  elif impl == 'simclrv1':\n",
        "    image = random_brightness(image, max_delta=max_delta)\n",
        "  else:\n",
        "    raise ValueError('Unknown impl {} for random brightness.'.format(impl))\n",
        "  return image\n",
        "\n",
        "\n",
        "def to_grayscale(image, keep_channels=True):\n",
        "  image = tf.image.rgb_to_grayscale(image)\n",
        "  if keep_channels:\n",
        "    image = tf.tile(image, [1, 1, 3])\n",
        "  return image\n",
        "\n",
        "\n",
        "def color_jitter(image,print(\"TensorFlow version:\", tf.__version__)\n",
        "                 strength,\n",
        "                 random_order=True):\n",
        "  \"\"\"Distorts the color of the image.\n",
        "  Args:\n",
        "    image: The input image tensor.\n",
        "    strength: the floating number for the strength of the color augmentation.\n",
        "    random_order: A bool, specifying whether to randomize the jittering order.\n",
        "  Returns:\n",
        "    The distorted image tensor.\n",
        "  \"\"\"\n",
        "  brightness = 0.8 * strength\n",
        "  contrast = 0.8 * strength\n",
        "  saturation = 0.8 * strength\n",
        "  hue = 0.2 * strength\n",
        "  if random_order:\n",
        "    return color_jitter_rand(image, brightness, contrast, saturation, hue)\n",
        "  else:\n",
        "    return color_jitter_nonrand(image, brightness, contrast, saturation, hue)\n",
        "\n",
        "\n",
        "def color_jitter_nonrand(image, brightness=0, contrast=0, saturation=0, hue=0):\n",
        "  \"\"\"Distorts the color of the image (jittering order is fixed).\n",
        "  Args:\n",
        "    image: The input image tensor.\n",
        "    brightness: A float, specifying the brightness for color jitter.\n",
        "    contrast: A float, specifying the contrast for color jitter.\n",
        "    saturation: A float, specifying the saturation for color jitter.\n",
        "    hue: A float, specifying the hue for color jitter.\n",
        "  Returns:\n",
        "    The distorted image tensor.\n",
        "  \"\"\"\n",
        "  with tf.name_scope('distort_color'):\n",
        "    def apply_transform(i, x, brightness, contrast, saturation, hue):\n",
        "      \"\"\"Apply the i-th transformation.\"\"\"\n",
        "      if brightness != 0 and i == 0:\n",
        "        x = random_brightness(x, max_delta=brightness)\n",
        "      elif contrast != 0 and i == 1:\n",
        "        x = tf.image.random_contrast(\n",
        "            x, lower=1-contrast, upper=1+contrast)\n",
        "      elif saturation != 0 and i == 2:\n",
        "        x = tf.image.random_saturation(\n",
        "            x, lower=1-saturation, upper=1+saturation)\n",
        "      elif hue != 0:\n",
        "        x = tf.image.random_hue(x, max_delta=hue)\n",
        "      return x\n",
        "\n",
        "    for i in range(4):\n",
        "      image = apply_transform(i, image, brightness, contrast, saturation, hue)\n",
        "      image = tf.clip_by_value(image, 0., 1.)\n",
        "    return image\n",
        "\n",
        "\n",
        "def color_jitter_rand(image, brightness=0, contrast=0, saturation=0, hue=0):\n",
        "  \"\"\"Distorts the color of the image (jittering order is random).\n",
        "  Args:\n",
        "    image: The input image tensor.\n",
        "    brightness: A float, specifying the brightness for color jitter.\n",
        "    contrast: A float, specifying the contrast for color jitter.\n",
        "    saturation: A float, specifying the saturation for color jitter.\n",
        "    hue: A float, specifying the hue for color jitter.\n",
        "  Returns:\n",
        "    The distorted image tensor.\n",
        "  \"\"\"\n",
        "  with tf.name_scope('distort_color'):\n",
        "    def apply_transform(i, x):\n",
        "      \"\"\"Apply the i-th transformation.\"\"\"\n",
        "      def brightness_foo():\n",
        "        if brightness == 0:\n",
        "          return x\n",
        "        else:\n",
        "          return random_brightness(x, max_delta=brightness)\n",
        "      def contrast_foo():\n",
        "        if contrast == 0:\n",
        "          return x\n",
        "        else:\n",
        "          return tf.image.random_contrast(x, lower=1-contrast, upper=1+contrast)\n",
        "      def saturation_foo():\n",
        "        if saturation == 0:\n",
        "          return x\n",
        "        else:\n",
        "          return tf.image.random_saturation(\n",
        "              x, lower=1-saturation, upper=1+saturation)\n",
        "      def hue_foo():\n",
        "        if hue == 0:\n",
        "          return x\n",
        "        else:\n",
        "          return tf.image.random_hue(x, max_delta=hue)\n",
        "      x = tf.cond(tf.less(i, 2),\n",
        "                  lambda: tf.cond(tf.less(i, 1), brightness_foo, contrast_foo),\n",
        "                  lambda: tf.cond(tf.less(i, 3), saturation_foo, hue_foo))\n",
        "      return x\n",
        "\n",
        "    perm = tf.random_shuffle(tf.range(4))\n",
        "    for i in range(4):\n",
        "      image = apply_transform(perm[i], image)\n",
        "      image = tf.clip_by_value(image, 0., 1.)\n",
        "    return image\n",
        "\n",
        "\n",
        "def _compute_crop_shape(\n",
        "    image_height, image_width, aspect_ratio, crop_proportion):\n",
        "  \"\"\"Compute aspect ratio-preserving shape for central crop.\n",
        "  The resulting shape retains `crop_proportion` along one side and a proportion\n",
        "  less than or equal to `crop_proportion` along the other side.\n",
        "  Args:\n",
        "    image_height: Height of image to be cropped.\n",
        "    image_width: Width of image to be cropped.\n",
        "    aspect_ratio: Desired aspect ratio (width / height) of output.\n",
        "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
        "  Returns:\n",
        "    crop_height: Height of image after cropping.\n",
        "    crop_width: Width of image after cropping.\n",
        "  \"\"\"\n",
        "  image_width_float = tf.cast(image_width, tf.float32)\n",
        "  image_height_float = tf.cast(image_height, tf.float32)\n",
        "\n",
        "  def _requested_aspect_ratio_wider_than_image():\n",
        "    crop_height = tf.cast(tf.math.rint(\n",
        "        crop_proportion / aspect_ratio * image_width_float), tf.int32)\n",
        "    crop_width = tf.cast(tf.math.rint(\n",
        "        crop_proportion * image_width_float), tf.int32)\n",
        "    return crop_height, crop_width\n",
        "\n",
        "  def _image_wider_than_requested_aspect_ratio():\n",
        "    crop_height = tf.cast(\n",
        "        tf.math.rint(crop_proportion * image_height_float), tf.int32)\n",
        "    crop_width = tf.cast(tf.math.rint(\n",
        "        crop_proportion * aspect_ratio *\n",
        "        image_height_float), tf.int32)\n",
        "    return crop_height, crop_width\n",
        "\n",
        "  return tf.cond(\n",
        "      aspect_ratio > image_width_float / image_height_float,\n",
        "      _requested_aspect_ratio_wider_than_image,\n",
        "      _image_wider_than_requested_aspect_ratio)\n",
        "\n",
        "\n",
        "def center_crop(image, height, width, crop_proportion):\n",
        "  \"\"\"Crops to center of image and rescales to desired size.\n",
        "  Args:\n",
        "    image: Image Tensor to crop.\n",
        "    height: Height of image to be cropped.\n",
        "    width: Width of image to be cropped.\n",
        "    crop_proportion: Proportion of image to retain along the less-cropped side.\n",
        "  Returns:\n",
        "    A `height` x `width` x channels Tensor holding a central crop of `image`.\n",
        "  \"\"\"\n",
        "  shape = tf.shape(image)\n",
        "  image_height = shape[0]\n",
        "  image_width = shape[1]\n",
        "  crop_height, crop_width = _compute_crop_shape(\n",
        "      image_height, image_width, height / width, crop_proportion)\n",
        "  offset_height = ((image_height - crop_height) + 1) // 2\n",
        "  offset_width = ((image_width - crop_width) + 1) // 2\n",
        "  image = tf.image.crop_to_bounding_box(\n",
        "      image, offset_height, offset_width, crop_height, crop_width)\n",
        "\n",
        "  image = tf.compat.v1.image.resize_bicubic([image], [height, width])[0]\n",
        "\n",
        "  return image\n",
        "\n",
        "\n",
        "def distorted_bounding_box_crop(image,\n",
        "                                bbox,\n",
        "                                min_object_covered=0.1,\n",
        "                                aspect_ratio_range=(0.75, 1.33),\n",
        "                                area_range=(0.05, 1.0),\n",
        "                                max_attempts=100,\n",
        "                                scope=None):\n",
        "  \"\"\"Generates cropped_image using one of the bboxes randomly distorted.\n",
        "  See `tf.image.sample_distorted_bounding_box` for more documentation.\n",
        "  Args:\n",
        "    image: `Tensor` of image data.\n",
        "    bbox: `Tensor` of bounding boxes arranged `[1, num_boxes, coords]`\n",
        "        where each coordinate is [0, 1) and the coordinates are arranged\n",
        "        as `[ymin, xmin, ymax, xmax]`. If num_boxes is 0 then use the whole\n",
        "        image.\n",
        "    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n",
        "        area of the image must contain at least this fraction of any bounding\n",
        "        box supplied.\n",
        "    aspect_ratio_range: An optional list of `float`s. The cropped area of the\n",
        "        image must have an aspect ratio = width / height within this range.\n",
        "    area_range: An optional list of `float`s. The cropped area of the image\n",
        "        must contain a fraction of the supplied image within in this range.\n",
        "    max_attempts: An optional `int`. Number of attempts at generating a cropped\n",
        "        region of the image of the specified constraints. After `max_attempts`\n",
        "        failures, return the entire image.\n",
        "    scope: Optional `str` for name scope.\n",
        "  Returns:\n",
        "    (cropped image `Tensor`, distorted bbox `Tensor`).\n",
        "  \"\"\"\n",
        "  with tf.name_scope('distorted_bounding_box_crop'):\n",
        "    shape = tf.shape(image)\n",
        "    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n",
        "        shape,\n",
        "        bounding_boxes=bbox,\n",
        "        min_object_covered=min_object_covered,\n",
        "        aspect_ratio_range=aspect_ratio_range,\n",
        "        area_range=area_range,\n",
        "        max_attempts=max_attempts,\n",
        "        use_image_if_no_bounding_boxes=True)\n",
        "    bbox_begin, bbox_size, _ = sample_distorted_bounding_box\n",
        "\n",
        "    # Crop the image to the specified bounding box.\n",
        "    offset_y, offset_x, _ = tf.unstack(bbox_begin)\n",
        "    target_height, target_width, _ = tf.unstack(bbox_size)\n",
        "    image = tf.image.crop_to_bounding_box(\n",
        "        image, offset_y, offset_x, target_height, target_width)\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def crop_and_resize(image, height, width):\n",
        "  \"\"\"Make a random crop and resize it to height `height` and width `width`.\n",
        "  Args:\n",
        "    image: Tensor representing the image.\n",
        "    height: Desired image height.\n",
        "    width: Desired image width.\n",
        "  Returns:\n",
        "    A `height` x `width` x channels Tensor holding a random crop of `image`.\n",
        "  \"\"\"\n",
        "  bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])\n",
        "  aspect_ratio = width / height\n",
        "  image = distorted_bounding_box_crop(\n",
        "      image,\n",
        "      bbox,\n",
        "      min_object_covered=0.1,\n",
        "      aspect_ratio_range=(3. / 4 * aspect_ratio, 4. / 3. * aspect_ratio),\n",
        "      area_range=(0.08, 1.0),\n",
        "      max_attempts=100,\n",
        "      scope=None)\n",
        "  return tf.compat.v1.image.resize_bicubic([image], [height, width])[0]\n",
        "\n",
        "\n",
        "def gaussian_blur(image, kernel_size, sigma, padding='SAME'):\n",
        "  \"\"\"Blurs the given image with separable convolution.\n",
        "  Args:\n",
        "    image: Tensor of shape [height, width, channels] and dtype float to blur.\n",
        "    kernel_size: Integer Tensor for the size of the blur kernel. This is should\n",
        "      be an odd number. If it is an even number, the actual kernel size will be\n",
        "      size + 1.\n",
        "    sigma: Sigma value for gaussian operator.\n",
        "    padding: Padding to use for the convolution. Typically 'SAME' or 'VALID'.\n",
        "  Returns:\n",
        "    A Tensor representing the blurred image.\n",
        "  \"\"\"\n",
        "  radius = tf.to_int32(kernel_size / 2)\n",
        "  kernel_size = radius * 2 + 1\n",
        "  x = tf.to_float(tf.range(-radius, radius + 1))\n",
        "  blur_filter = tf.exp(\n",
        "      -tf.pow(x, 2.0) / (2.0 * tf.pow(tf.to_float(sigma), 2.0)))\n",
        "  blur_filter /= tf.reduce_sum(blur_filter)\n",
        "  # One vertical and one horizontal filter.\n",
        "  blur_v = tf.reshape(blur_filter, [kernel_size, 1, 1, 1])\n",
        "  blur_h = tf.reshape(blur_filter, [1, kernel_size, 1, 1])\n",
        "  num_channels = tf.shape(image)[-1]\n",
        "  blur_h = tf.tile(blur_h, [1, 1, num_channels, 1])\n",
        "  blur_v = tf.tile(blur_v, [1, 1, num_channels, 1])\n",
        "  expand_batch_dim = image.shape.ndims == 3\n",
        "  if expand_batch_dim:\n",
        "    # Tensorflow requires batched input to convolutions, which we can fake with\n",
        "    # an extra dimension.\n",
        "    image = tf.expand_dims(image, axis=0)\n",
        "  blurred = tf.nn.depthwise_conv2d(\n",
        "      image, blur_h, strides=[1, 1, 1, 1], padding=padding)\n",
        "  blurred = tf.nn.depthwise_conv2d(\n",
        "      blurred, blur_v, strides=[1, 1, 1, 1], padding=padding)\n",
        "  if expand_batch_dim:\n",
        "    blurred = tf.squeeze(blurred, axis=0)\n",
        "  return blurred\n",
        "\n",
        "\n",
        "def random_crop_with_resize(image, height, width, p=1.0):\n",
        "  \"\"\"Randomly crop and resize an image.\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    height: Height of output image.\n",
        "    width: Width of output image.\n",
        "    p: Probability of applying this transformation.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor`.\n",
        "  \"\"\"\n",
        "  def _transform(image):  # pylint: disable=missing-docstring\n",
        "    image = crop_and_resize(image, height, width)\n",
        "    return image\n",
        "  return random_apply(_transform, p=p, x=image)\n",
        "\n",
        "\n",
        "def random_color_jitter(image, p=1.0):\n",
        "  def _transform(image):\n",
        "    color_jitter_t = functools.partial(\n",
        "        color_jitter, strength=FLAGS_color_jitter_strength)\n",
        "    image = random_apply(color_jitter_t, p=0.8, x=image)\n",
        "    return random_apply(to_grayscale, p=0.2, x=image)\n",
        "  return random_apply(_transform, p=p, x=image)\n",
        "\n",
        "\n",
        "def random_blur(image, height, width, p=1.0):\n",
        "  \"\"\"Randomly blur an image.\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    height: Height of output image.\n",
        "    width: Width of output image.\n",
        "    p: probability of applying this transformation.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor`.\n",
        "  \"\"\"\n",
        "  del width\n",
        "  def _transform(image):\n",
        "    sigma = tf.random.uniform([], 0.1, 2.0, dtype=tf.float32)\n",
        "    return gaussian_blur(\n",
        "        image, kernel_size=height//10, sigma=sigma, padding='SAME')\n",
        "  return random_apply(_transform, p=p, x=image)\n",
        "\n",
        "\n",
        "def batch_random_blur(images_list, height, width, blur_probability=0.5):\n",
        "  \"\"\"Apply efficient batch data transformations.\n",
        "  Args:\n",
        "    images_list: a list of image tensors.\n",
        "    height: the height of image.\n",
        "    width: the width of image.\n",
        "    blur_probability: the probaility to apply the blur operator.\n",
        "  Returns:\n",
        "    Preprocessed feature list.\n",
        "  \"\"\"\n",
        "  def generate_selector(p, bsz):\n",
        "    shape = [bsz, 1, 1, 1]\n",
        "    selector = tf.cast(\n",
        "        tf.less(tf.random.uniform(shape, 0, 1, dtype=tf.float32), p),\n",
        "        tf.float32)\n",
        "    return selector\n",
        "\n",
        "  new_images_list = []\n",
        "  for images in images_list:\n",
        "    images_new = random_blur(images, height, width, p=1.)\n",
        "    selector = generate_selector(blur_probability, tf.shape(images)[0])\n",
        "    images = images_new * selector + images * (1 - selector)\n",
        "    images = tf.clip_by_value(images, 0., 1.)\n",
        "    new_images_list.append(images)\n",
        "\n",
        "  return new_images_list\n",
        "\n",
        "\n",
        "def preprocess_for_train(image, height, width,\n",
        "                         color_distort=True, crop=True, flip=True):\n",
        "  \"\"\"Preprocesses the given image for training.\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    height: Height of output image.\n",
        "    width: Width of output image.\n",
        "    color_distort: Whether to apply the color distortion.\n",
        "    crop: Whether to crop the image.\n",
        "    flip: Whether or not to flip left and right of an image.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor`.\n",
        "  \"\"\"\n",
        "  if crop:\n",
        "    image = random_crop_with_resize(image, height, width)\n",
        "  if flip:\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "  if color_distort:\n",
        "    image = random_color_jitter(image)\n",
        "  image = tf.reshape(image, [height, width, 3])\n",
        "  image = tf.clip_by_value(image, 0., 1.)\n",
        "  return image\n",
        "\n",
        "\n",
        "def preprocess_for_eval(image, height, width, crop=True):\n",
        "  \"\"\"Preprocesses the given image for evaluation.\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    height: Height of output image.\n",
        "    width: Width of output image.\n",
        "    crop: Whether or not to (center) crop the test images.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor`.\n",
        "  \"\"\"\n",
        "  if crop:\n",
        "    image = center_crop(image, height, width, crop_proportion=CROP_PROPORTION)\n",
        "  image = tf.reshape(image, [height, width, 3])\n",
        "  image = tf.clip_by_value(image, 0., 1.)\n",
        "  return image\n",
        "\n",
        "\n",
        "def preprocess_image(image, height, width, is_training=False,\n",
        "                     color_distort=True, test_crop=True):\n",
        "  \"\"\"Preprocesses the given image.\n",
        "  Args:\n",
        "    image: `Tensor` representing an image of arbitrary size.\n",
        "    height: Height of output image.\n",
        "    width: Width of output image.\n",
        "    is_training: `bool` for whether the preprocessing is for training.\n",
        "    color_distort: whether to apply the color distortion.\n",
        "    test_crop: whether or not to extract a central crop of the images\n",
        "        (as for standard ImageNet evaluation) during the evaluation.\n",
        "  Returns:\n",
        "    A preprocessed image `Tensor` of range [0, 1].\n",
        "  \"\"\"\n",
        "  image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "  if is_training:\n",
        "    return preprocess_for_train(image, height, width, color_distort)\n",
        "  else:\n",
        "    return preprocess_for_eval(image, height, width, test_crop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.11.1\n"
          ]
        }
      ],
      "source": [
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "both",
        "id": "hhXTUoUs_9zd"
      },
      "outputs": [],
      "source": [
        "EETA_DEFAULT = 0.001\n",
        "\n",
        "\n",
        "class LARSOptimizer(tf.keras.optimizers.Optimizer):\n",
        "  \"\"\"Layer-wise Adaptive Rate Scaling for large batch training.\n",
        "\n",
        "  Introduced by \"Large Batch Training of Convolutional Networks\" by Y. You,\n",
        "  I. Gitman, and B. Ginsburg. (https://arxiv.org/abs/1708.03888)\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               learning_rate,\n",
        "               momentum=0.9,\n",
        "               use_nesterov=False,\n",
        "               weight_decay=0.0,\n",
        "               exclude_from_weight_decay=None,\n",
        "               exclude_from_layer_adaptation=None,\n",
        "               classic_momentum=True,\n",
        "               eeta=EETA_DEFAULT,\n",
        "               name=\"LARSOptimizer\"):\n",
        "    \"\"\"Constructs a LARSOptimizer.\n",
        "\n",
        "    Args:\n",
        "      learning_rate: A `float` for learning rate.\n",
        "      momentum: A `float` for momentum.\n",
        "      use_nesterov: A 'Boolean' for whether to use nesterov momentum.\n",
        "      weight_decay: A `float` for weight decay.\n",
        "      exclude_from_weight_decay: A list of `string` for variable screening, if\n",
        "          any of the string appears in a variable's name, the variable will be\n",
        "          excluded for computing weight decay. For example, one could specify\n",
        "          the list like ['batch_normalization', 'bias'] to exclude BN and bias\n",
        "          from weight decay.\n",
        "      exclude_from_layer_adaptation: Similar to exclude_from_weight_decay, but\n",
        "          for layer adaptation. If it is None, it will be defaulted the same as\n",
        "          exclude_from_weight_decay.\n",
        "      classic_momentum: A `boolean` for whether to use classic (or popular)\n",
        "          momentum. The learning rate is applied during momeuntum update in\n",
        "          classic momentum, but after momentum for popular momentum.\n",
        "      eeta: A `float` for scaling of learning rate when computing trust ratio.\n",
        "      name: The name for the scope.\n",
        "    \"\"\"\n",
        "    super(LARSOptimizer, self).__init__(name)\n",
        "\n",
        "    # self._set_hyper(\"learning_rate\", learning_rate)\n",
        "    self.momentum = momentum\n",
        "    self.weight_decay = weight_decay\n",
        "    self.use_nesterov = use_nesterov\n",
        "    self.classic_momentum = classic_momentum\n",
        "    self.eeta = eeta\n",
        "    self.exclude_from_weight_decay = exclude_from_weight_decay\n",
        "    # exclude_from_layer_adaptation is set to exclude_from_weight_decay if the\n",
        "    # arg is None.\n",
        "    if exclude_from_layer_adaptation:\n",
        "      self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n",
        "    else:\n",
        "      self.exclude_from_layer_adaptation = exclude_from_weight_decay\n",
        "\n",
        "  def _create_slots(self, var_list):\n",
        "    for v in var_list:\n",
        "      self.add_slot(v, \"Momentum\")\n",
        "\n",
        "  def _resource_apply_dense(self, grad, param, apply_state=None):\n",
        "    if grad is None or param is None:\n",
        "      return tf.no_op()\n",
        "\n",
        "    var_device, var_dtype = param.device, param.dtype.base_dtype\n",
        "    coefficients = ((apply_state or {}).get((var_device, var_dtype)) or\n",
        "                    self._fallback_apply_state(var_device, var_dtype))\n",
        "    learning_rate = coefficients[\"lr_t\"]\n",
        "\n",
        "    param_name = param.name\n",
        "\n",
        "    v = self.get_slot(param, \"Momentum\")\n",
        "\n",
        "    if self._use_weight_decay(param_name):\n",
        "      grad += self.weight_decay * param\n",
        "\n",
        "    if self.classic_momentum:\n",
        "      trust_ratio = 1.0\n",
        "      if self._do_layer_adaptation(param_name):\n",
        "        w_norm = tf.norm(param, ord=2)\n",
        "        g_norm = tf.norm(grad, ord=2)\n",
        "        trust_ratio = tf.where(\n",
        "            tf.greater(w_norm, 0),\n",
        "            tf.where(tf.greater(g_norm, 0), (self.eeta * w_norm / g_norm), 1.0),\n",
        "            1.0)\n",
        "      scaled_lr = learning_rate * trust_ratio\n",
        "\n",
        "      next_v = tf.multiply(self.momentum, v) + scaled_lr * grad\n",
        "      if self.use_nesterov:\n",
        "        update = tf.multiply(self.momentum, next_v) + scaled_lr * grad\n",
        "      else:\n",
        "        update = next_v\n",
        "      next_param = param - update\n",
        "    else:\n",
        "      next_v = tf.multiply(self.momentum, v) + grad\n",
        "      if self.use_nesterov:\n",
        "        update = tf.multiply(self.momentum, next_v) + grad\n",
        "      else:\n",
        "        update = next_v\n",
        "\n",
        "      trust_ratio = 1.0\n",
        "      if self._do_layer_adaptation(param_name):\n",
        "        w_norm = tf.norm(param, ord=2)\n",
        "        v_norm = tf.norm(update, ord=2)\n",
        "        trust_ratio = tf.where(\n",
        "            tf.greater(w_norm, 0),\n",
        "            tf.where(tf.greater(v_norm, 0), (self.eeta * w_norm / v_norm), 1.0),\n",
        "            1.0)\n",
        "      scaled_lr = trust_ratio * learning_rate\n",
        "      next_param = param - scaled_lr * update\n",
        "\n",
        "    return tf.group(*[\n",
        "        param.assign(next_param, use_locking=False),\n",
        "        v.assign(next_v, use_locking=False)\n",
        "    ])\n",
        "\n",
        "  def _use_weight_decay(self, param_name):\n",
        "    \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
        "    if not self.weight_decay:\n",
        "      return False\n",
        "    if self.exclude_from_weight_decay:\n",
        "      for r in self.exclude_from_weight_decay:\n",
        "        if re.search(r, param_name) is not None:\n",
        "          return False\n",
        "    return True\n",
        "\n",
        "  def _do_layer_adaptation(self, param_name):\n",
        "    \"\"\"Whether to do layer-wise learning rate adaptation for `param_name`.\"\"\"\n",
        "    if self.exclude_from_layer_adaptation:\n",
        "      for r in self.exclude_from_layer_adaptation:\n",
        "        if re.search(r, param_name) is not None:\n",
        "          return False\n",
        "    return True\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(LARSOptimizer, self).get_config()\n",
        "    config.update({\n",
        "        \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n",
        "        \"momentum\": self.momentum,\n",
        "        \"classic_momentum\": self.classic_momentum,\n",
        "        \"weight_decay\": self.weight_decay,\n",
        "        \"eeta\": self.eeta,\n",
        "        \"use_nesterov\": self.use_nesterov,\n",
        "    })\n",
        "    return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JelWN4TMUSSY"
      },
      "outputs": [],
      "source": [
        "#@title Defining the student architecture and distillation loss.\n",
        "\n",
        "def add_kd_loss(student_logits, teacher_logits, temperature):\n",
        "  \"\"\"Compute distillation loss.\"\"\"\n",
        "  teacher_probs = tf.nn.softmax(teacher_logits / temperature)\n",
        "  kd_loss = tf.reduce_mean(temperature**2 * tf.nn.softmax_cross_entropy_with_logits(\n",
        "      teacher_probs, student_logits / temperature))\n",
        "  return kd_loss\n",
        "\n",
        "# Define a small student ConvNet\n",
        "def build_student_model():\n",
        "  student_model = tf.keras.models.Sequential()\n",
        "  student_model.add(tf.keras.layers.Conv2D(64, (3, 3), input_shape=(224, 224, 3)))\n",
        "  student_model.add(tf.keras.layers.BatchNormalization())\n",
        "  student_model.add(tf.keras.layers.Activation('relu'))\n",
        "  student_model.add(tf.keras.layers.MaxPooling2D((4, 4)))\n",
        "  student_model.add(tf.keras.layers.Conv2D(128, (3, 3)))\n",
        "  student_model.add(tf.keras.layers.BatchNormalization())\n",
        "  student_model.add(tf.keras.layers.Activation('relu'))\n",
        "  student_model.add(tf.keras.layers.MaxPooling2D((4, 4)))\n",
        "  student_model.add(tf.keras.layers.Conv2D(256, (3, 3)))\n",
        "  student_model.add(tf.keras.layers.BatchNormalization())\n",
        "  student_model.add(tf.keras.layers.Activation('relu'))\n",
        "  student_model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
        "  student_model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
        "  student_model.add(tf.keras.layers.Dense(1000))\n",
        "  return student_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_preprocess(image_path,label):\n",
        "    image = pydicom.dcmread(image_path.numpy().decode())\n",
        "    pixel_array = image.pixel_array.astype(np.float32)\n",
        "    normalized_pixel_array = pixel_array / np.max(pixel_array)\n",
        "    dicom_tensor = tf.convert_to_tensor(normalized_pixel_array)\n",
        "    expanded_tensor = tf.expand_dims(dicom_tensor, axis=-1)  # Expand along the first axis\n",
        "    expanded_tensor = tf.tile(expanded_tensor, [1, 1, 3]) \n",
        "    return expanded_tensor\n",
        "\n",
        "def create_dataset():\n",
        "    label_names = {'A':0,'B':1,'E':2,'G':3}\n",
        "    image_paths  = glob.glob(os.path.join('/media/wenuka/New Volume-G/01.FYP/Dataset/Lung_CT_Dataset/Lung-PET-CT-Dx-NBIA-Manifest-122220/Lung-PET-CT-Dx', '**', '*.dcm'), recursive=True)\n",
        "    labels = [label_names[x.split('/')[9][8]] for x in image_paths]\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_paths,labels))\n",
        "    dataset = dataset.map(lambda x, y: (tf.py_function(load_and_preprocess, [x, y], tf.float32), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "9a64a5d7fef24ad5bcf392a4b02c3fde",
            "8434e4617b384154892ad54555eaeb70",
            "fa31befef1654609addb84189785f7a4",
            "670298453eca420f9fcbaa626ad67d85",
            "ea6d22f449834822afae5b48ba9284b8",
            "943ccbfe32454f17b53192e0c04d1638",
            "b8d7a56ddb464a0eaa67bcd61f27783b",
            "f43fefa9223e4105ac0e38e73017e5cb",
            "ffce6a69c8074e30a77bd058a7bdb7f3",
            "d96f7b0ef2c44ee58049efbb0fa05665",
            "e51b0fc9529c452ea30c1e65a27f5524"
          ]
        },
        "id": "MDCY4h7bHxj8",
        "outputId": "d2251aa7-1918-4f0f-99e9-4dd97e23033a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type <class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "in user code:\n\n\n    TypeError: outer_factory.<locals>.inner_factory.<locals>.tf___preprocess() takes 1 positional argument but 2 were given\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m   x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m preprocess_image(\n\u001b[1;32m     10\u001b[0m       x[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, color_distort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m---> 12\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mtfds_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_preprocess\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mbatch(batch_size)\n",
            "File \u001b[0;32m~/.pyenv/versions/simclr/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:2294\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2291\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m deterministic \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m DEBUG_MODE:\n\u001b[1;32m   2292\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2293\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`num_parallel_calls` argument is specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2294\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMapDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_cardinality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2295\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2296\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m ParallelMapDataset(\n\u001b[1;32m   2297\u001b[0m       \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2298\u001b[0m       map_func,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2301\u001b[0m       preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2302\u001b[0m       name\u001b[38;5;241m=\u001b[39mname)\n",
            "File \u001b[0;32m~/.pyenv/versions/simclr/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:5499\u001b[0m, in \u001b[0;36mMapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m   5497\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_inter_op_parallelism \u001b[38;5;241m=\u001b[39m use_inter_op_parallelism\n\u001b[1;32m   5498\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality \u001b[38;5;241m=\u001b[39m preserve_cardinality\n\u001b[0;32m-> 5499\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5501\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5503\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_legacy_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_legacy_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5504\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[1;32m   5505\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m gen_dataset_ops\u001b[38;5;241m.\u001b[39mmap_dataset(\n\u001b[1;32m   5506\u001b[0m     input_dataset\u001b[38;5;241m.\u001b[39m_variant_tensor,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   5507\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5510\u001b[0m     preserve_cardinality\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preserve_cardinality,\n\u001b[1;32m   5511\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_common_args)\n",
            "File \u001b[0;32m~/.pyenv/versions/simclr/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:263\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    257\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    259\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    261\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
            "File \u001b[0;32m~/.pyenv/versions/simclr/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:226\u001b[0m, in \u001b[0;36mTracingCompiler.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    218\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m      `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    229\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\n",
            "File \u001b[0;32m~/.pyenv/versions/simclr/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:192\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mvalidate_inputs_with_signature(args, kwargs)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 192\u001b[0m   concrete_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_concrete_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m   seen_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    194\u001b[0m   captured \u001b[38;5;241m=\u001b[39m object_identity\u001b[38;5;241m.\u001b[39mObjectIdentitySet(\n\u001b[1;32m    195\u001b[0m       concrete_function\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39minternal_captures)\n",
            "File \u001b[0;32m~/.pyenv/versions/simclr/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:157\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m   args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    155\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/.pyenv/versions/simclr/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:360\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m   \u001b[38;5;66;03m# Only get placeholders for arguments, not captures\u001b[39;00m\n\u001b[1;32m    358\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m generalized_func_key\u001b[38;5;241m.\u001b[39m_placeholder_value()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m concrete_function\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39m_capture_func_lib  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;66;03m# Maintain the list of all captures\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/simclr/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:284\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[1;32m    281\u001b[0m ]\n\u001b[1;32m    282\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[1;32m    283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m monomorphic_function\u001b[38;5;241m.\u001b[39mConcreteFunction(\n\u001b[0;32m--> 284\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    294\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[1;32m    295\u001b[0m     spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m    300\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concrete_function\n",
            "File \u001b[0;32m~/.pyenv/versions/simclr/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1283\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1281\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1283\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
            "File \u001b[0;32m~/.pyenv/versions/simclr/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:240\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;129m@eager_function\u001b[39m\u001b[38;5;241m.\u001b[39mdefun_with_attributes(\n\u001b[1;32m    235\u001b[0m     input_signature\u001b[38;5;241m=\u001b[39mstructure\u001b[38;5;241m.\u001b[39mget_flat_tensor_specs(\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_structure),\n\u001b[1;32m    237\u001b[0m     autograph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    238\u001b[0m     attributes\u001b[38;5;241m=\u001b[39mdefun_kwargs)\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):  \u001b[38;5;66;03m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m   ret \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_structure, ret)\n\u001b[1;32m    242\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m [ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m ret]\n",
            "File \u001b[0;32m~/.pyenv/versions/simclr/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:171\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[1;32m    170\u001b[0m   nested_args \u001b[38;5;241m=\u001b[39m (nested_args,)\n\u001b[0;32m--> 171\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mautograph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtf_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_ctx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnested_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m ret \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(ret)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _should_pack(ret):\n",
            "File \u001b[0;32m~/.pyenv/versions/simclr/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:692\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 692\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m    693\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/simclr/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 689\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
            "File \u001b[0;32m~/.pyenv/versions/simclr/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconverted_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meffective_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args)\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n\n    TypeError: outer_factory.<locals>.inner_factory.<locals>.tf___preprocess() takes 1 positional argument but 2 were given\n"
          ]
        }
      ],
      "source": [
        "#@title Load tensorflow datasets: we use tensorflow flower dataset as an example\n",
        "\n",
        "batch_size = 16\n",
        "dataset_name = 'tf_flowers'\n",
        "\n",
        "tfds_dataset = create_dataset().prefetch(100)\n",
        "print(\"Type\",type(tfds_dataset))\n",
        "def _preprocess(x):\n",
        "  x[0] = preprocess_image(\n",
        "      x[0], 224, 224, is_training=True, color_distort=False)\n",
        "  return x\n",
        "ds = tfds_dataset.map(_preprocess).batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train': <PrefetchDataset element_spec={'image': TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None)}>}\n"
          ]
        }
      ],
      "source": [
        "print(tfds_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WXspghpERRG",
        "outputId": "7f47ddeb-67d9-44f2-dfaa-f6a9a7597dda"
      },
      "outputs": [],
      "source": [
        "#@title Load module and construct the computation graph\n",
        "\n",
        "total_steps = 10\n",
        "learning_rate = 2.0\n",
        "momentum = 0.9\n",
        "weight_decay = 1e-4\n",
        "temperature = 1.\n",
        "\n",
        "class Model(tf.keras.Model):\n",
        "  def __init__(self, path):\n",
        "    super(Model, self).__init__()\n",
        "    self.teacher_saved_model = tf.saved_model.load(path)\n",
        "    self.student_model = build_student_model()\n",
        "    self.dense_layer = tf.keras.layers.Dense(units=num_classes, name=\"head_supervised_new\")\n",
        "    learning_rate_schedule = tf.keras.experimental.CosineDecay(learning_rate,\n",
        "                                                               total_steps)\n",
        "    self.optimizer = LARSOptimizer(\n",
        "      learning_rate_schedule,\n",
        "      momentum=momentum,\n",
        "      weight_decay=weight_decay,\n",
        "      exclude_from_weight_decay=['batch_normalization', 'bias'])\n",
        "\n",
        "  def call(self, x):\n",
        "    with tf.GradientTape() as tape:\n",
        "      outputs = self.teacher_saved_model(x['image'], trainable=False)\n",
        "      teacher_logits_t = outputs['logits_sup']\n",
        "      student_logits_t = self.student_model(x['image'])\n",
        "      loss_t = add_kd_loss(student_logits_t, teacher_logits_t, temperature)\n",
        "\n",
        "      trainable_weights = self.student_model.trainable_weights\n",
        "      print('Variables to train:', [v.name for v in trainable_weights])\n",
        "      grads = tape.gradient(loss_t, trainable_weights)\n",
        "      self.optimizer.apply_gradients(zip(grads, trainable_weights))\n",
        "    return loss_t, x[\"image\"], teacher_logits_t, student_logits_t\n",
        "\n",
        "model = Model(\"gs://simclr-checkpoints-tf2/simclrv2/finetuned_100pct/r50_1x_sk0/saved_model/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVVfjd_ShREj",
        "outputId": "3503b338-bb9b-4298-fd28-92f5e1fe2907"
      },
      "outputs": [],
      "source": [
        "tfds_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHSQuEwnCBKN",
        "outputId": "cdc27d9c-e8bb-40f5-ffc9-4bc82a7091b4"
      },
      "outputs": [],
      "source": [
        "#@title We train the student based on teacher's output for just a few iterations.\n",
        "iterator = iter(ds)\n",
        "for it in range(total_steps):\n",
        "  x = next(iterator)\n",
        "  print(x)\n",
        "  break\n",
        "  # loss, image, teacher_logits, student_logits = model(x)\n",
        "  # teacher_logits = teacher_logits.numpy()\n",
        "  # student_logits = student_logits.numpy()\n",
        "  # labels = teacher_logits.argmax(-1)\n",
        "  # pred = student_logits.argmax(-1)\n",
        "  # correct = np.sum(pred == labels)\n",
        "  # total = labels.size\n",
        "  # print(\"[Iter {}] Loss: {} Top 1: {}\".format(it+1, loss, correct/float(total)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "height": 867
        },
        "id": "FOhK8anzK21K",
        "outputId": "0bd3742b-2259-4b9c-f625-69006bceb5ce"
      },
      "outputs": [],
      "source": [
        "#@title Plot the images and predictions (of ImageNet classes) from teacher & student\n",
        "# Note: The quality of predictions are expected to be poor here due to:\n",
        "# (1) teacher has not been fine-tuned to fine-grained flower classes, so its prediction is constrained by ImageNet classes.\n",
        "# (2) the student network is very small, has not been well tuned, neither converged yet.\n",
        "fig, axes = plt.subplots(5, 1, figsize=(15, 15))\n",
        "for i in range(5):\n",
        "  axes[i].imshow(image[i])\n",
        "  true_text = imagenet_int_to_str[labels[i]]\n",
        "  pred_text = imagenet_int_to_str[pred[i]]\n",
        "  axes[i].axis('off')\n",
        "  axes[i].text(256, 128, 'Teacher: ' + true_text + '\\n' + 'Student: ' + pred_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXV8vygYl6WC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "TF2 distillation_self_training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "670298453eca420f9fcbaa626ad67d85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d96f7b0ef2c44ee58049efbb0fa05665",
            "placeholder": "​",
            "style": "IPY_MODEL_e51b0fc9529c452ea30c1e65a27f5524",
            "value": " 5/5 [00:02&lt;00:00,  1.40 file/s]"
          }
        },
        "8434e4617b384154892ad54555eaeb70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_943ccbfe32454f17b53192e0c04d1638",
            "placeholder": "​",
            "style": "IPY_MODEL_b8d7a56ddb464a0eaa67bcd61f27783b",
            "value": "Dl Completed...: 100%"
          }
        },
        "943ccbfe32454f17b53192e0c04d1638": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a64a5d7fef24ad5bcf392a4b02c3fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8434e4617b384154892ad54555eaeb70",
              "IPY_MODEL_fa31befef1654609addb84189785f7a4",
              "IPY_MODEL_670298453eca420f9fcbaa626ad67d85"
            ],
            "layout": "IPY_MODEL_ea6d22f449834822afae5b48ba9284b8"
          }
        },
        "b8d7a56ddb464a0eaa67bcd61f27783b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d96f7b0ef2c44ee58049efbb0fa05665": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e51b0fc9529c452ea30c1e65a27f5524": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea6d22f449834822afae5b48ba9284b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f43fefa9223e4105ac0e38e73017e5cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa31befef1654609addb84189785f7a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f43fefa9223e4105ac0e38e73017e5cb",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ffce6a69c8074e30a77bd058a7bdb7f3",
            "value": 5
          }
        },
        "ffce6a69c8074e30a77bd058a7bdb7f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
